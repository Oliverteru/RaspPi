{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1221_0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Oliverteru/image_recognition/blob/master/1221_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hOF2q7zBr9vf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GjZ6qn8owMqC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-P_QNiOiA5TD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "RM2WYrdDAyVC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNWA7NHjwub9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p9c3y_qZwvUA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id = \"1Ca4LM7Ls-xU_LdDASzv9SlJJblLMaEbk\"\n",
        "downloaded = drive.CreateFile({'id': id})\n",
        "downloaded.GetContentFile('keras-yolo3.zip')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8Vj0N2FzxTY",
        "colab_type": "code",
        "outputId": "3e06269c-a317-40ae-bb01-28bccb737fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  keras-yolo3.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uLe3R_TY1eKf",
        "colab_type": "code",
        "outputId": "cd68df90-1bdf-47c1-e746-7ce502845189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip keras-yolo3.zip  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  keras-yolo3.zip\n",
            "   creating: keras-yolo3/\n",
            "  inflating: keras-yolo3/yolo_video.py  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/keras-yolo3/\n",
            "  inflating: __MACOSX/keras-yolo3/._yolo_video.py  \n",
            "  inflating: keras-yolo3/darknet53.cfg  \n",
            "  inflating: __MACOSX/keras-yolo3/._darknet53.cfg  \n",
            "   creating: keras-yolo3/yolo3/\n",
            " extracting: keras-yolo3/yolo3/__init__.py  \n",
            "   creating: __MACOSX/keras-yolo3/yolo3/\n",
            "  inflating: __MACOSX/keras-yolo3/yolo3/.___init__.py  \n",
            "  inflating: keras-yolo3/yolo3/model.py  \n",
            "  inflating: __MACOSX/keras-yolo3/yolo3/._model.py  \n",
            "  inflating: keras-yolo3/yolo3/utils.py  \n",
            "  inflating: __MACOSX/keras-yolo3/yolo3/._utils.py  \n",
            "  inflating: __MACOSX/keras-yolo3/._yolo3  \n",
            "  inflating: keras-yolo3/yolov3-tiny.weights  \n",
            "  inflating: __MACOSX/keras-yolo3/._yolov3-tiny.weights  \n",
            "  inflating: keras-yolo3/LICENSE     \n",
            "  inflating: __MACOSX/keras-yolo3/._LICENSE  \n",
            "  inflating: keras-yolo3/coco_annotation.py  \n",
            "  inflating: __MACOSX/keras-yolo3/._coco_annotation.py  \n",
            "   creating: keras-yolo3/model_data/\n",
            "  inflating: keras-yolo3/model_data/voc_classes.txt  \n",
            "   creating: __MACOSX/keras-yolo3/model_data/\n",
            "  inflating: __MACOSX/keras-yolo3/model_data/._voc_classes.txt  \n",
            "  inflating: keras-yolo3/model_data/yolo_anchors.txt  \n",
            "  inflating: __MACOSX/keras-yolo3/model_data/._yolo_anchors.txt  \n",
            "  inflating: keras-yolo3/model_data/coco_classes.txt  \n",
            "  inflating: __MACOSX/keras-yolo3/model_data/._coco_classes.txt  \n",
            "  inflating: keras-yolo3/model_data/tiny_yolo_anchors.txt  \n",
            "  inflating: __MACOSX/keras-yolo3/model_data/._tiny_yolo_anchors.txt  \n",
            "  inflating: __MACOSX/keras-yolo3/._model_data  \n",
            "  inflating: keras-yolo3/voc_annotation.py  \n",
            "  inflating: __MACOSX/keras-yolo3/._voc_annotation.py  \n",
            "  inflating: keras-yolo3/yolov3-tiny.cfg  \n",
            "  inflating: __MACOSX/keras-yolo3/._yolov3-tiny.cfg  \n",
            "  inflating: keras-yolo3/convert.py  \n",
            "  inflating: __MACOSX/keras-yolo3/._convert.py  \n",
            "  inflating: keras-yolo3/yolov3.cfg  \n",
            "  inflating: __MACOSX/keras-yolo3/._yolov3.cfg  \n",
            "  inflating: keras-yolo3/train_bottleneck.py  \n",
            "  inflating: __MACOSX/keras-yolo3/._train_bottleneck.py  \n",
            "  inflating: keras-yolo3/kmeans.py   \n",
            "  inflating: __MACOSX/keras-yolo3/._kmeans.py  \n",
            "  inflating: keras-yolo3/README.md   \n",
            "  inflating: __MACOSX/keras-yolo3/._README.md  \n",
            "  inflating: keras-yolo3/.gitignore  \n",
            "  inflating: __MACOSX/keras-yolo3/._.gitignore  \n",
            "  inflating: keras-yolo3/train.py    \n",
            "  inflating: __MACOSX/keras-yolo3/._train.py  \n",
            "  inflating: keras-yolo3/yolo.py     \n",
            "  inflating: __MACOSX/keras-yolo3/._yolo.py  \n",
            "   creating: keras-yolo3/font/\n",
            "  inflating: keras-yolo3/font/SIL Open Font License.txt  \n",
            "   creating: __MACOSX/keras-yolo3/font/\n",
            "  inflating: __MACOSX/keras-yolo3/font/._SIL Open Font License.txt  \n",
            "  inflating: keras-yolo3/font/FiraMono-Medium.otf  \n",
            "  inflating: __MACOSX/keras-yolo3/font/._FiraMono-Medium.otf  \n",
            "  inflating: __MACOSX/keras-yolo3/._font  \n",
            "  inflating: __MACOSX/._keras-yolo3  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CodyBNDk1jSK",
        "colab_type": "code",
        "outputId": "7fd8aa77-4ed0-4ac6-c424-416e218a6625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  keras-yolo3  keras-yolo3.zip\t__MACOSX  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n4626Yhp1lm1",
        "colab_type": "code",
        "outputId": "a6afb573-a039-46fa-fa66-79ad51ccd972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%cd keras-yolo3"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-yolo3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nB4dssYJH-NU",
        "colab_type": "code",
        "outputId": "a44e05b2-9598-47fe-a7b2-82e4e85febd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2468
        }
      },
      "cell_type": "code",
      "source": [
        "%run convert.py yolov3-tiny.cfg yolov3-tiny.weights model_data/tiny_yolo_weights.h5"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading weights.\n",
            "Weights Header:  0 2 0 [32013312]\n",
            "Parsing Darknet config.\n",
            "Creating Keras model.\n",
            "Parsing section net_0\n",
            "Parsing section convolutional_0\n",
            "conv2d bn leaky (3, 3, 3, 16)\n",
            "Parsing section maxpool_0\n",
            "Parsing section convolutional_1\n",
            "conv2d bn leaky (3, 3, 16, 32)\n",
            "Parsing section maxpool_1\n",
            "Parsing section convolutional_2\n",
            "conv2d bn leaky (3, 3, 32, 64)\n",
            "Parsing section maxpool_2\n",
            "Parsing section convolutional_3\n",
            "conv2d bn leaky (3, 3, 64, 128)\n",
            "Parsing section maxpool_3\n",
            "Parsing section convolutional_4\n",
            "conv2d bn leaky (3, 3, 128, 256)\n",
            "Parsing section maxpool_4\n",
            "Parsing section convolutional_5\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section maxpool_5\n",
            "Parsing section convolutional_6\n",
            "conv2d bn leaky (3, 3, 512, 1024)\n",
            "Parsing section convolutional_7\n",
            "conv2d bn leaky (1, 1, 1024, 256)\n",
            "Parsing section convolutional_8\n",
            "conv2d bn leaky (3, 3, 256, 512)\n",
            "Parsing section convolutional_9\n",
            "conv2d    linear (1, 1, 512, 255)\n",
            "Parsing section yolo_0\n",
            "Parsing section route_0\n",
            "Parsing section convolutional_10\n",
            "conv2d bn leaky (1, 1, 256, 128)\n",
            "Parsing section upsample_0\n",
            "Parsing section route_1\n",
            "Concatenating route layers: [<tf.Tensor 'up_sampling2d_1/ResizeNearestNeighbor:0' shape=(?, ?, ?, 128) dtype=float32>, <tf.Tensor 'leaky_re_lu_5/LeakyRelu:0' shape=(?, ?, ?, 256) dtype=float32>]\n",
            "Parsing section convolutional_11\n",
            "conv2d bn leaky (3, 3, 384, 256)\n",
            "Parsing section convolutional_12\n",
            "conv2d    linear (1, 1, 256, 255)\n",
            "Parsing section yolo_1\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, None, 3 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 1 432         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 1 64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 3 4608        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 3 128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 3 0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 6 18432       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 6 256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 6 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, None, None, 6 0           leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 1 73728       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 1 512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, None, None, 1 0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 2 294912      max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 2 1024        conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, None, None, 2 0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 5 1179648     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, None, 5 2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, None, None, 5 0           leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 1 4718592     max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, None, None, 1 4096        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, None, None, 1 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, None, None, 2 262144      leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, None, None, 2 1024        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 2 0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, None, None, 1 32768       leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, None, None, 1 512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, None, None, 1 0           leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 3 0           up_sampling2d_1[0][0]            \n",
            "                                                                 leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, None, None, 5 1179648     leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, None, None, 2 884736      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, None, None, 5 2048        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, None, None, 2 1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 5 0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, None, None, 2 130815      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, None, None, 2 65535       leaky_re_lu_11[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 8,858,734\n",
            "Trainable params: 8,852,366\n",
            "Non-trainable params: 6,368\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Saved Keras model to model_data/tiny_yolo_weights.h5\n",
            "Read 8858734 of 8858734.0 from Darknet weights.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pc8TBIwIIBxg",
        "colab_type": "code",
        "outputId": "9441b5c0-6931-4359-d75b-b6b9d78285d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!ls model_data\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "coco_classes.txt       tiny_yolo_weights.h5  yolo_anchors.txt\n",
            "tiny_yolo_anchors.txt  voc_classes.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_l_Fp8yRIOO2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "aaf52d46-cc15-4bc4-b1ec-e85ed0cf382f"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "coco_annotation.py  LICENSE\t\t train.py\t    yolov3-tiny.cfg\n",
            "convert.py\t    model_data\t\t voc_annotation.py  yolov3-tiny.weights\n",
            "darknet53.cfg\t    README.md\t\t yolo3\t\t    yolo_video.py\n",
            "font\t\t    tiny_yolo.py\t yolo.py\n",
            "kmeans.py\t    train_bottleneck.py  yolov3.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "psaHokWfBqDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp yolo.py tiny_yolo.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fze1pLxfMvsE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3621
        },
        "outputId": "6add1ddc-cadb-4db9-a014-694dd00a4317"
      },
      "cell_type": "code",
      "source": [
        "!cat tiny_yolo.py"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# -*- coding: utf-8 -*-\n",
            "\"\"\"\n",
            "Class definition of YOLO_v3 style detection model on image and video\n",
            "\"\"\"\n",
            "\n",
            "import colorsys\n",
            "import os\n",
            "from timeit import default_timer as timer\n",
            "\n",
            "import numpy as np\n",
            "from keras import backend as K\n",
            "from keras.models import load_model\n",
            "from keras.layers import Input\n",
            "from PIL import Image, ImageFont, ImageDraw\n",
            "\n",
            "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
            "from yolo3.utils import letterbox_image\n",
            "import os\n",
            "from keras.utils import multi_gpu_model\n",
            "\n",
            "class YOLO(object):\n",
            "    _defaults = {\n",
            "        \"model_path\": 'model_data/yolo.h5',\n",
            "        \"anchors_path\": 'model_data/yolo_anchors.txt',\n",
            "        \"classes_path\": 'model_data/coco_classes.txt',\n",
            "        \"score\" : 0.3,\n",
            "        \"iou\" : 0.45,\n",
            "        \"model_image_size\" : (416, 416),\n",
            "        \"gpu_num\" : 1,\n",
            "    }\n",
            "\n",
            "    @classmethod\n",
            "    def get_defaults(cls, n):\n",
            "        if n in cls._defaults:\n",
            "            return cls._defaults[n]\n",
            "        else:\n",
            "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
            "\n",
            "    def __init__(self, **kwargs):\n",
            "        self.__dict__.update(self._defaults) # set up default values\n",
            "        self.__dict__.update(kwargs) # and update with user overrides\n",
            "        self.class_names = self._get_class()\n",
            "        self.anchors = self._get_anchors()\n",
            "        self.sess = K.get_session()\n",
            "        self.boxes, self.scores, self.classes = self.generate()\n",
            "\n",
            "    def _get_class(self):\n",
            "        classes_path = os.path.expanduser(self.classes_path)\n",
            "        with open(classes_path) as f:\n",
            "            class_names = f.readlines()\n",
            "        class_names = [c.strip() for c in class_names]\n",
            "        return class_names\n",
            "\n",
            "    def _get_anchors(self):\n",
            "        anchors_path = os.path.expanduser(self.anchors_path)\n",
            "        with open(anchors_path) as f:\n",
            "            anchors = f.readline()\n",
            "        anchors = [float(x) for x in anchors.split(',')]\n",
            "        return np.array(anchors).reshape(-1, 2)\n",
            "\n",
            "    def generate(self):\n",
            "        model_path = os.path.expanduser(self.model_path)\n",
            "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
            "\n",
            "        # Load model, or construct model and load weights.\n",
            "        num_anchors = len(self.anchors)\n",
            "        num_classes = len(self.class_names)\n",
            "        is_tiny_version = num_anchors==6 # default setting\n",
            "        try:\n",
            "            self.yolo_model = load_model(model_path, compile=False)\n",
            "        except:\n",
            "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
            "                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
            "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
            "        else:\n",
            "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
            "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
            "                'Mismatch between model and given anchor and class sizes'\n",
            "\n",
            "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
            "\n",
            "        # Generate colors for drawing bounding boxes.\n",
            "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
            "                      for x in range(len(self.class_names))]\n",
            "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
            "        self.colors = list(\n",
            "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
            "                self.colors))\n",
            "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
            "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
            "        np.random.seed(None)  # Reset seed to default.\n",
            "\n",
            "        # Generate output tensor targets for filtered bounding boxes.\n",
            "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
            "        if self.gpu_num>=2:\n",
            "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
            "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
            "                len(self.class_names), self.input_image_shape,\n",
            "                score_threshold=self.score, iou_threshold=self.iou)\n",
            "        return boxes, scores, classes\n",
            "\n",
            "    def detect_image(self, image):\n",
            "        start = timer()\n",
            "\n",
            "        if self.model_image_size != (None, None):\n",
            "            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
            "            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
            "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
            "        else:\n",
            "            new_image_size = (image.width - (image.width % 32),\n",
            "                              image.height - (image.height % 32))\n",
            "            boxed_image = letterbox_image(image, new_image_size)\n",
            "        image_data = np.array(boxed_image, dtype='float32')\n",
            "\n",
            "        print(image_data.shape)\n",
            "        image_data /= 255.\n",
            "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
            "\n",
            "        out_boxes, out_scores, out_classes = self.sess.run(\n",
            "            [self.boxes, self.scores, self.classes],\n",
            "            feed_dict={\n",
            "                self.yolo_model.input: image_data,\n",
            "                self.input_image_shape: [image.size[1], image.size[0]],\n",
            "                K.learning_phase(): 0\n",
            "            })\n",
            "\n",
            "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
            "\n",
            "        font = ImageFont.truetype(font='font/FiraMono-Medium.otf',\n",
            "                    size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
            "        thickness = (image.size[0] + image.size[1]) // 300\n",
            "\n",
            "        for i, c in reversed(list(enumerate(out_classes))):\n",
            "            predicted_class = self.class_names[c]\n",
            "            box = out_boxes[i]\n",
            "            score = out_scores[i]\n",
            "\n",
            "            label = '{} {:.2f}'.format(predicted_class, score)\n",
            "            draw = ImageDraw.Draw(image)\n",
            "            label_size = draw.textsize(label, font)\n",
            "\n",
            "            top, left, bottom, right = box\n",
            "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
            "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
            "            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
            "            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
            "            print(label, (left, top), (right, bottom))\n",
            "\n",
            "            if top - label_size[1] >= 0:\n",
            "                text_origin = np.array([left, top - label_size[1]])\n",
            "            else:\n",
            "                text_origin = np.array([left, top + 1])\n",
            "\n",
            "            # My kingdom for a good redistributable image drawing library.\n",
            "            for i in range(thickness):\n",
            "                draw.rectangle(\n",
            "                    [left + i, top + i, right - i, bottom - i],\n",
            "                    outline=self.colors[c])\n",
            "            draw.rectangle(\n",
            "                [tuple(text_origin), tuple(text_origin + label_size)],\n",
            "                fill=self.colors[c])\n",
            "            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
            "            del draw\n",
            "\n",
            "        end = timer()\n",
            "        print(end - start)\n",
            "        return image\n",
            "\n",
            "    def close_session(self):\n",
            "        self.sess.close()\n",
            "\n",
            "def detect_video(yolo, video_path, output_path=\"\"):\n",
            "    import cv2\n",
            "    vid = cv2.VideoCapture(video_path)\n",
            "    if not vid.isOpened():\n",
            "        raise IOError(\"Couldn't open webcam or video\")\n",
            "    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
            "    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n",
            "    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
            "                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
            "    isOutput = True if output_path != \"\" else False\n",
            "    if isOutput:\n",
            "        print(\"!!! TYPE:\", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\n",
            "        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n",
            "    accum_time = 0\n",
            "    curr_fps = 0\n",
            "    fps = \"FPS: ??\"\n",
            "    prev_time = timer()\n",
            "    while True:\n",
            "        return_value, frame = vid.read()\n",
            "        image = Image.fromarray(frame)\n",
            "        image = yolo.detect_image(image)\n",
            "        result = np.asarray(image)\n",
            "        curr_time = timer()\n",
            "        exec_time = curr_time - prev_time\n",
            "        prev_time = curr_time\n",
            "        accum_time = accum_time + exec_time\n",
            "        curr_fps = curr_fps + 1\n",
            "        if accum_time > 1:\n",
            "            accum_time = accum_time - 1\n",
            "            fps = \"FPS: \" + str(curr_fps)\n",
            "            curr_fps = 0\n",
            "        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
            "                    fontScale=0.50, color=(255, 0, 0), thickness=2)\n",
            "        cv2.namedWindow(\"result\", cv2.WINDOW_NORMAL)\n",
            "        cv2.imshow(\"result\", result)\n",
            "        if isOutput:\n",
            "            out.write(result)\n",
            "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
            "            break\n",
            "    yolo.close_session()\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y6eCqJEXL_ai",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ddeba34-548c-4aef-e0b0-82fe3fca49a6"
      },
      "cell_type": "code",
      "source": [
        "%%writefile tiny_yolo.py \n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Class definition of YOLO_v3 style detection model on image and video\n",
        "\"\"\"\n",
        "\n",
        "import colorsys\n",
        "import os\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.layers import Input\n",
        "from PIL import Image, ImageFont, ImageDraw\n",
        "\n",
        "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
        "from yolo3.utils import letterbox_image\n",
        "import os\n",
        "from keras.utils import multi_gpu_model\n",
        "\n",
        "class YOLO(object):\n",
        "    _defaults = {\n",
        "        \"model_path\": 'model_data/tiny_yolo_weights.h5',      # 'model_data/yolo.h5''\n",
        "        \"anchors_path\": 'model_data/tiny_yolo_anchors.txt',  #  yolo_anchors.txt'\n",
        "        \"classes_path\": 'model_data/coco_classes.txt',\n",
        "        \"score\" : 0.3,\n",
        "        \"iou\" : 0.45,\n",
        "        \"model_image_size\" : (416, 416),\n",
        "        \"gpu_num\" : 1,\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def get_defaults(cls, n):\n",
        "        if n in cls._defaults:\n",
        "            return cls._defaults[n]\n",
        "        else:\n",
        "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(self._defaults) # set up default values\n",
        "        self.__dict__.update(kwargs) # and update with user overrides\n",
        "        self.class_names = self._get_class()\n",
        "        self.anchors = self._get_anchors()\n",
        "        self.sess = K.get_session()\n",
        "        self.boxes, self.scores, self.classes = self.generate()\n",
        "\n",
        "    def _get_class(self):\n",
        "        classes_path = os.path.expanduser(self.classes_path)\n",
        "        with open(classes_path) as f:\n",
        "            class_names = f.readlines()\n",
        "        class_names = [c.strip() for c in class_names]\n",
        "        return class_names\n",
        "\n",
        "    def _get_anchors(self):\n",
        "        anchors_path = os.path.expanduser(self.anchors_path)\n",
        "        with open(anchors_path) as f:\n",
        "            anchors = f.readline()\n",
        "        anchors = [float(x) for x in anchors.split(',')]\n",
        "        return np.array(anchors).reshape(-1, 2)\n",
        "\n",
        "    def generate(self):\n",
        "        model_path = os.path.expanduser(self.model_path)\n",
        "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
        "\n",
        "        # Load model, or construct model and load weights.\n",
        "        num_anchors = len(self.anchors)\n",
        "        num_classes = len(self.class_names)\n",
        "        is_tiny_version = num_anchors==6 # default setting\n",
        "        try:\n",
        "            self.yolo_model = load_model(model_path, compile=False)\n",
        "        except:\n",
        "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
        "                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
        "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
        "        else:\n",
        "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
        "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
        "                'Mismatch between model and given anchor and class sizes'\n",
        "\n",
        "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
        "\n",
        "        # Generate colors for drawing bounding boxes.\n",
        "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
        "                      for x in range(len(self.class_names))]\n",
        "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "        self.colors = list(\n",
        "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
        "                self.colors))\n",
        "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
        "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
        "        np.random.seed(None)  # Reset seed to default.\n",
        "\n",
        "        # Generate output tensor targets for filtered bounding boxes.\n",
        "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
        "        if self.gpu_num>=2:\n",
        "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
        "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
        "                len(self.class_names), self.input_image_shape,\n",
        "                score_threshold=self.score, iou_threshold=self.iou)\n",
        "        return boxes, scores, classes\n",
        "\n",
        "    def detect_image(self, image):\n",
        "        start = timer()\n",
        "\n",
        "        if self.model_image_size != (None, None):\n",
        "            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
        "            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
        "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
        "        else:\n",
        "            new_image_size = (image.width - (image.width % 32),\n",
        "                              image.height - (image.height % 32))\n",
        "            boxed_image = letterbox_image(image, new_image_size)\n",
        "        image_data = np.array(boxed_image, dtype='float32')\n",
        "\n",
        "        print(image_data.shape)\n",
        "        image_data /= 255.\n",
        "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
        "\n",
        "        out_boxes, out_scores, out_classes = self.sess.run(\n",
        "            [self.boxes, self.scores, self.classes],\n",
        "            feed_dict={\n",
        "                self.yolo_model.input: image_data,\n",
        "                self.input_image_shape: [image.size[1], image.size[0]],\n",
        "                K.learning_phase(): 0\n",
        "            })\n",
        "\n",
        "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
        "\n",
        "        font = ImageFont.truetype(font='font/FiraMono-Medium.otf',\n",
        "                    size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
        "        thickness = (image.size[0] + image.size[1]) // 300\n",
        "\n",
        "        for i, c in reversed(list(enumerate(out_classes))):\n",
        "            predicted_class = self.class_names[c]\n",
        "            box = out_boxes[i]\n",
        "            score = out_scores[i]\n",
        "\n",
        "            label = '{} {:.2f}'.format(predicted_class, score)\n",
        "            draw = ImageDraw.Draw(image)\n",
        "            label_size = draw.textsize(label, font)\n",
        "\n",
        "            top, left, bottom, right = box\n",
        "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
        "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
        "            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
        "            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
        "            print(label, (left, top), (right, bottom))\n",
        "\n",
        "            if top - label_size[1] >= 0:\n",
        "                text_origin = np.array([left, top - label_size[1]])\n",
        "            else:\n",
        "                text_origin = np.array([left, top + 1])\n",
        "\n",
        "            # My kingdom for a good redistributable image drawing library.\n",
        "            for i in range(thickness):\n",
        "                draw.rectangle(\n",
        "                    [left + i, top + i, right - i, bottom - i],\n",
        "                    outline=self.colors[c])\n",
        "            draw.rectangle(\n",
        "                [tuple(text_origin), tuple(text_origin + label_size)],\n",
        "                fill=self.colors[c])\n",
        "            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "            del draw\n",
        "\n",
        "        end = timer()\n",
        "        print(end - start)\n",
        "        return image\n",
        "\n",
        "    def close_session(self):\n",
        "        self.sess.close()\n",
        "\n",
        "def detect_video(yolo, video_path, output_path=\"\"):\n",
        "    import cv2\n",
        "    vid = cv2.VideoCapture(video_path)\n",
        "    if not vid.isOpened():\n",
        "        raise IOError(\"Couldn't open webcam or video\")\n",
        "    video_FourCC    = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
        "    video_fps       = vid.get(cv2.CAP_PROP_FPS)\n",
        "    video_size      = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
        "                        int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "    isOutput = True if output_path != \"\" else False\n",
        "    if isOutput:\n",
        "        print(\"!!! TYPE:\", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\n",
        "        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n",
        "    accum_time = 0\n",
        "    curr_fps = 0\n",
        "    fps = \"FPS: ??\"\n",
        "    prev_time = timer()\n",
        "    while True:\n",
        "        return_value, frame = vid.read()\n",
        "        image = Image.fromarray(frame)\n",
        "        image = yolo.detect_image(image)\n",
        "        result = np.asarray(image)\n",
        "        curr_time = timer()\n",
        "        exec_time = curr_time - prev_time\n",
        "        prev_time = curr_time\n",
        "        accum_time = accum_time + exec_time\n",
        "        curr_fps = curr_fps + 1\n",
        "        if accum_time > 1:\n",
        "            accum_time = accum_time - 1\n",
        "            fps = \"FPS: \" + str(curr_fps)\n",
        "            curr_fps = 0\n",
        "        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    fontScale=0.50, color=(255, 0, 0), thickness=2)\n",
        "        cv2.namedWindow(\"result\", cv2.WINDOW_NORMAL)\n",
        "        cv2.imshow(\"result\", result)\n",
        "        if isOutput:\n",
        "            out.write(result)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    yolo.close_session()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting tiny_yolo.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3m3NJ0OKOXop",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp yolo_video.py tiny_yolo_video.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dM_t8lvZOnwE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1326
        },
        "outputId": "a2b2ede6-a9ac-42d0-d6e5-011ba520ced3"
      },
      "cell_type": "code",
      "source": [
        "!cat tiny_yolo_video.py"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import sys\n",
            "import argparse\n",
            "from yolo import YOLO, detect_video\n",
            "from PIL import Image\n",
            "\n",
            "def detect_img(yolo):\n",
            "    while True:\n",
            "        img = input('Input image filename:')\n",
            "        try:\n",
            "            image = Image.open(img)\n",
            "        except:\n",
            "            print('Open Error! Try again!')\n",
            "            continue\n",
            "        else:\n",
            "            r_image = yolo.detect_image(image)\n",
            "            r_image.show()\n",
            "    yolo.close_session()\n",
            "\n",
            "FLAGS = None\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    # class YOLO defines the default value, so suppress any default here\n",
            "    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)\n",
            "    '''\n",
            "    Command line options\n",
            "    '''\n",
            "    parser.add_argument(\n",
            "        '--model', type=str,\n",
            "        help='path to model weight file, default ' + YOLO.get_defaults(\"model_path\")\n",
            "    )\n",
            "\n",
            "    parser.add_argument(\n",
            "        '--anchors', type=str,\n",
            "        help='path to anchor definitions, default ' + YOLO.get_defaults(\"anchors_path\")\n",
            "    )\n",
            "\n",
            "    parser.add_argument(\n",
            "        '--classes', type=str,\n",
            "        help='path to class definitions, default ' + YOLO.get_defaults(\"classes_path\")\n",
            "    )\n",
            "\n",
            "    parser.add_argument(\n",
            "        '--gpu_num', type=int,\n",
            "        help='Number of GPU to use, default ' + str(YOLO.get_defaults(\"gpu_num\"))\n",
            "    )\n",
            "\n",
            "    parser.add_argument(\n",
            "        '--image', default=False, action=\"store_true\",\n",
            "        help='Image detection mode, will ignore all positional arguments'\n",
            "    )\n",
            "    '''\n",
            "    Command line positional arguments -- for video detection mode\n",
            "    '''\n",
            "    parser.add_argument(\n",
            "        \"--input\", nargs='?', type=str,required=False,default='./path2your_video',\n",
            "        help = \"Video input path\"\n",
            "    )\n",
            "\n",
            "    parser.add_argument(\n",
            "        \"--output\", nargs='?', type=str, default=\"\",\n",
            "        help = \"[Optional] Video output path\"\n",
            "    )\n",
            "\n",
            "    FLAGS = parser.parse_args()\n",
            "\n",
            "    if FLAGS.image:\n",
            "        \"\"\"\n",
            "        Image detection mode, disregard any remaining command line arguments\n",
            "        \"\"\"\n",
            "        print(\"Image detection mode\")\n",
            "        if \"input\" in FLAGS:\n",
            "            print(\" Ignoring remaining command line arguments: \" + FLAGS.input + \",\" + FLAGS.output)\n",
            "        detect_img(YOLO(**vars(FLAGS)))\n",
            "    elif \"input\" in FLAGS:\n",
            "        detect_video(YOLO(**vars(FLAGS)), FLAGS.input, FLAGS.output)\n",
            "    else:\n",
            "        print(\"Must specify at least video_input_path.  See usage with --help.\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_o0RaEXyOsDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad21f433-f6bf-4ad4-f3fd-d378c1f2055b"
      },
      "cell_type": "code",
      "source": [
        "%%writefile tiny_yolo_video.py\n",
        "import sys\n",
        "import argparse\n",
        "from tiny_yolo import YOLO, detect_video\n",
        "from PIL import Image\n",
        "\n",
        "def detect_img(yolo):\n",
        "    while True:\n",
        "        img = input('Input image filename:')\n",
        "        try:\n",
        "            image = Image.open(img)\n",
        "        except:\n",
        "            print('Open Error! Try again!')\n",
        "            continue\n",
        "        else:\n",
        "            r_image = yolo.detect_image(image)\n",
        "            r_image.show()\n",
        "    yolo.close_session()\n",
        "\n",
        "FLAGS = None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # class YOLO defines the default value, so suppress any default here\n",
        "    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS)\n",
        "    '''\n",
        "    Command line options\n",
        "    '''\n",
        "    parser.add_argument(\n",
        "        '--model', type=str,\n",
        "        help='path to model weight file, default ' + YOLO.get_defaults(\"model_path\")\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--anchors', type=str,\n",
        "        help='path to anchor definitions, default ' + YOLO.get_defaults(\"anchors_path\")\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--classes', type=str,\n",
        "        help='path to class definitions, default ' + YOLO.get_defaults(\"classes_path\")\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--gpu_num', type=int,\n",
        "        help='Number of GPU to use, default ' + str(YOLO.get_defaults(\"gpu_num\"))\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--image', default=False, action=\"store_true\",\n",
        "        help='Image detection mode, will ignore all positional arguments'\n",
        "    )\n",
        "    '''\n",
        "    Command line positional arguments -- for video detection mode\n",
        "    '''\n",
        "    parser.add_argument(\n",
        "        \"--input\", nargs='?', type=str,required=False,default='./path2your_video',\n",
        "        help = \"Video input path\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--output\", nargs='?', type=str, default=\"\",\n",
        "        help = \"[Optional] Video output path\"\n",
        "    )\n",
        "\n",
        "    FLAGS = parser.parse_args()\n",
        "\n",
        "    if FLAGS.image:\n",
        "        \"\"\"\n",
        "        Image detection mode, disregard any remaining command line arguments\n",
        "        \"\"\"\n",
        "        print(\"Image detection mode\")\n",
        "        if \"input\" in FLAGS:\n",
        "            print(\" Ignoring remaining command line arguments: \" + FLAGS.input + \",\" + FLAGS.output)\n",
        "        detect_img(YOLO(**vars(FLAGS)))\n",
        "    elif \"input\" in FLAGS:\n",
        "        detect_video(YOLO(**vars(FLAGS)), FLAGS.input, FLAGS.output)\n",
        "    else:\n",
        "        print(\"Must specify at least video_input_path.  See usage with --help.\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting tiny_yolo_video.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BULnDZpESAXs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}